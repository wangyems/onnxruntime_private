parameters:
- name: CudaVersion
  type: string
  default: '11.8'
- name: docker_base_image
  type: string
- name: buildJava
  type: boolean
- name: buildNodejs
  type: boolean

stages:
- stage: Linux_C_API_Packaging_GPU
  dependsOn: []
  jobs:
  - template: ../templates/c-api-gpu.yml
    parameters:
      CudaVersion: ${{ parameters.CudaVersion }}
      buildJava: ${{ parameters.buildJava }}
      EP: cuda

  - template: ../templates/c-api-gpu.yml
    parameters:
      CudaVersion: ${{ parameters.CudaVersion }}
      buildJava: ${{ parameters.buildJava }}
      buildNodejs: ${{ parameters.buildNodejs }}
      EP: tensorrt

  - job: Linux_Packaging_combined_CUDA
    condition: succeeded()
    dependsOn:
    - Linux_C_API_Packaging_cuda
    - Linux_C_API_Packaging_tensorrt
    workspace:
      clean: all
    pool: 'Onnxruntime-Linux-GPU'
    variables:
    - name: CUDA_VERSION_MAJOR
      ${{ if eq(parameters.CudaVersion, '11.8') }}:
        value: '11'
      ${{ if eq(parameters.CudaVersion, '12.2') }}:
        value: '12'
    - name: linux_trt_version
      ${{ if eq(parameters.CudaVersion, '11.8') }}:
        value: 10.0.1.6-1.cuda11.8
      ${{ if eq(parameters.CudaVersion, '12.2') }}:
        value: 10.0.1.6-1.cuda12.4
    steps:
    - checkout: self                           # due to checkout multiple repos, the root directory is $(Build.SourcesDirectory)/onnxruntime
      submodules: false
    - checkout: onnxruntime-inference-examples # due to checkout multiple repos, the root directory is $(Build.SourcesDirectory)/onnxruntime-inference-examples
      submodules: false

    - task: mspremier.PostBuildCleanup.PostBuildCleanup-task.PostBuildCleanup@3
      displayName: 'Clean Agent Directories'
      condition: always()

    - template: ../templates/get-docker-image-steps.yml
      parameters:
        ScriptName: $(Build.SourcesDirectory)/onnxruntime/tools/ci_build/get_docker_image.py
        Dockerfile: $(Build.SourcesDirectory)/onnxruntime/tools/ci_build/github/linux/docker/inference/x86_64/default/cuda${{ variables.CUDA_VERSION_MAJOR }}/Dockerfile
        Context: $(Build.SourcesDirectory)/onnxruntime/tools/ci_build/github/linux/docker/inference/x86_64/default/cuda${{ variables.CUDA_VERSION_MAJOR }}
        DockerBuildArgs: "--build-arg BASEIMAGE=${{ parameters.docker_base_image }} --build-arg TRT_VERSION=${{ variables.linux_trt_version }} --build-arg BUILD_UID=$( id -u )"
        Repository: onnxruntimetensorrt$(CUDA_VERSION_MAJOR)build
        UpdateDepsTxt: false

    - template: ../templates/set-version-number-variables-step.yml
      parameters:
        versionFileDirectory: '$(Build.SourcesDirectory)/onnxruntime'
        workingDirectory: '$(Build.SourcesDirectory)/onnxruntime'
    - task: DownloadPipelineArtifact@2
      displayName: 'Download Pipeline Artifact - Combined GPU'
      inputs:
        artifactName: 'onnxruntime-linux-x64-cuda'
        targetPath: '$(Build.BinariesDirectory)/tgz-artifacts'

    - task: DownloadPipelineArtifact@2
      displayName: 'Download Pipeline Artifact - Combined GPU'
      inputs:
        artifactName: 'onnxruntime-linux-x64-tensorrt'
        targetPath: '$(Build.BinariesDirectory)/tgz-artifacts'

    - task: ShellScript@2
      displayName: 'Shell Script'
      inputs:
        scriptPath: 'onnxruntime/tools/ci_build/github/linux/extract_and_bundle_gpu_package.sh'
        args: '-a $(Build.BinariesDirectory)/tgz-artifacts'
        workingDirectory: '$(Build.BinariesDirectory)/tgz-artifacts'

    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.BinariesDirectory)/tgz-artifacts/onnxruntime-linux-x64-gpu'
        includeRootFolder: false
        archiveType: 'tar' # Options: zip, 7z, tar, wim
        tarCompression: 'gz'
        archiveFile: '$(Build.ArtifactStagingDirectory)/onnxruntime-linux-x64-gpu-$(OnnxRuntimeVersion).tgz'
        replaceExistingArchive: true

    - template: ../templates/validate-package.yml
      parameters:
        PackageType: 'tarball'
        PackagePath: '$(Build.ArtifactStagingDirectory)'
        PackageName: 'onnxruntime-linux-x64-gpu-$(OnnxRuntimeVersion).tgz'
        ScriptPath: '$(Build.SourcesDirectory)/onnxruntime/tools/nuget/validate_package.py'
        PlatformsSupported: 'linux-x64'
        VerifyNugetSigning: false
        workingDirectory: '$(Build.ArtifactStagingDirectory)'


    - task: CmdLine@2
      displayName: 'Test C API application for GPU package'
      inputs:
        script: |
          docker run --gpus all -e CFLAGS="-Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -fstack-clash-protection -fcf-protection -O3 -Wl,--strip-all" -e CXXFLAGS="-Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -fstack-clash-protection -fcf-protection -O3 -Wl,--strip-all" -e NVIDIA_VISIBLE_DEVICES=all --rm --volume /data/models:/data/models --volume $(Build.SourcesDirectory):/src_dir \
          --volume $(Build.ArtifactStagingDirectory):/artifact_src -e NIGHTLY_BUILD onnxruntimetensorrt$(CUDA_VERSION_MAJOR)build \
          /src_dir/onnxruntime-inference-examples/c_cxx/squeezenet/run_capi_application.sh -o /src_dir/onnxruntime -p /artifact_src/onnxruntime-linux-x64-gpu-$(OnnxRuntimeVersion).tgz -w /src_dir/onnxruntime-inference-examples/c_cxx/squeezenet
        workingDirectory: '$(Build.ArtifactStagingDirectory)'

    - task: PublishPipelineArtifact@1
      inputs:
        targetPath: '$(Build.ArtifactStagingDirectory)/onnxruntime-linux-x64-gpu-$(OnnxRuntimeVersion).tgz'
        artifactName: 'onnxruntime-linux-x64-gpu'
    - template: ../templates/component-governance-component-detection-steps.yml
      parameters:
        condition: 'succeeded'
